# Customer Data Lakehouse ETL Pipeline

This project simulates a real-world data engineering scenario where customer, asset, and product lifecycle data are ingested from multiple sources, transformed, validated, and loaded into a unified analytics-ready data lakehouse.

The solution is designed to showcase modern data engineering practices using open-source tools like **Apache Airflow**, **PySpark**, **Delta Lake**, and **Python**.

---

## ğŸ” Project Overview

**Goal:**  
Unify fragmented customer and asset data from internal databases, third-party APIs, and CSV files into a clean, reliable, and scalable data lakehouse system.

---

## ğŸ“Š Data Sources

| Source Type      | Description                                      |
|------------------|--------------------------------------------------|
| PostgreSQL        | Internal customer profiles and usage metrics     |
| REST API          | External warranty and lifecycle details          |
| CSV Files         | Weekly asset exports uploaded by teams           |

---

## âš™ï¸ Architecture
```
       +------------------+       +-----------------+
       |   PostgreSQL     |       |    API Source    |
       +------------------+       +-----------------+
               |                         |
        extract_postgres.py      extract_api.py
               |                         |
               +-----------+-------------+
                           |
                     [Raw Zone Storage]
                           |
                      ingest_csv.py
                           |
                    +-------------+
                    | Apache Airflow|
                    +-------------+
                           |
                 transform_assets_spark.py
                           |
                 [Delta Lake / SQLite Warehouse]
                           |
                 validate_data.py (Quality Checks)
                           |
                    Optional: Streamlit Dashboard
```
---

## ğŸ§° Tools & Technologies

- **Airflow** â€“ Workflow orchestration
- **Python** â€“ Ingestion scripts & API integration
- **PySpark** â€“ Transformations and joins
- **Delta Lake** or **SQLite** â€“ Storage (depending on local or cloud environment)
- **Great Expectations** â€“ Data quality validation
- **Streamlit** â€“ Optional interactive dashboard

---

## ğŸ—‚ï¸ Repository Structure
```
customer-data-lakehouse/
â”œâ”€â”€ dags/                      # Airflow DAGs
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ extract_postgres.py
â”‚   â”œâ”€â”€ extract_api.py
â”‚   â””â”€â”€ ingest_csv.py
â”œâ”€â”€ transformation/
â”‚   â””â”€â”€ transform_assets_spark.py
â”œâ”€â”€ data_quality/
â”‚   â””â”€â”€ validate_data.py
â”œâ”€â”€ warehouse/
â”‚   â””â”€â”€ tables/
â”œâ”€â”€ dashboard/
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ architecture_diagram.png
```

---

## âœ… Key Features

- ğŸ”„ Automated ETL pipeline using Airflow
- ğŸ§¹ Data standardization, schema alignment, and cleaning with PySpark
- âœ”ï¸ Data quality checks using custom validation rules
- ğŸ“¦ Modular and scalable codebase ready for production deployment
- ğŸ“ˆ Optional dashboard to visualize final tables

---

## ğŸš€ Getting Started

### Prerequisites
- Python 3.8+
- Apache Airflow
- Spark + Delta Lake
- PostgreSQL (local or hosted)
- Docker (optional for Airflow setup)

### Installation

```bash
# Clone the repo
git clone https://github.com/yourusername/customer-data-lakehouse.git
cd customer-data-lakehouse

# Set up Python environment
pip install -r requirements.txt

# Start Airflow (if using standalone setup)
airflow standalone
```
## ğŸ“Œ Example Use Cases
	â€¢	Unified view of customer health, asset age, and lifecycle status
	â€¢	Early detection of expiring warranties
	â€¢	Streamlined reporting for customer success and product teams


